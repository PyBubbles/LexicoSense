{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dedb802-82a5-473e-9ae1-6b57ec965e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "2024-12-01 18:25:30 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844bdc9b0ae6424fa5e817e507fcd282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 18:25:55 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-12-01 18:25:55 WARNING: GPU requested, but is not available!\n",
      "2024-12-01 18:25:55 INFO: Using device: cpu\n",
      "2024-12-01 18:25:55 INFO: Loading: tokenize\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-01 18:25:59 INFO: Loading: mwt\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-01 18:25:59 INFO: Loading: pos\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:262: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-01 18:26:00 INFO: Loading: lemma\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:227: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-01 18:26:00 INFO: Loading: depparse\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\depparse\\trainer.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-01 18:26:01 INFO: Loading: ner\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\ner\\trainer.py:171: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-01 18:26:02 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from PyQt5.QtWidgets import (\n",
    "    QApplication, QMainWindow, QPushButton, QLabel, QVBoxLayout, \n",
    "    QHBoxLayout, QWidget, QLineEdit, QFileDialog, QTextEdit, QMessageBox\n",
    ")\n",
    "from PyQt5.QtCore import Qt\n",
    "from PyQt5.QtGui import QFont\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "import spacy_stanza\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "#farasa POS tagger\n",
    "pos_tagger = FarasaPOSTagger()\n",
    "\n",
    "#initialize the farasa segmenter, to perform segmentation\n",
    "segmenter = FarasaSegmenter()\n",
    "\n",
    "# load arabic NLP pipeline from stanza\n",
    "nlp = spacy_stanza.load_pipeline(\"ar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b7e0a7-20e0-4fbd-9e7b-02791edc96b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-01 18:26:14,859 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "2024-12-01 18:26:34 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d3c8fab6cc48278999f9e782ed46f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 18:26:36 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-12-01 18:26:36 WARNING: GPU requested, but is not available!\n",
      "2024-12-01 18:26:36 INFO: Using device: cpu\n",
      "2024-12-01 18:26:36 INFO: Loading: tokenize\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-01 18:26:36 INFO: Loading: mwt\n",
      "2024-12-01 18:26:36 INFO: Loading: pos\n",
      "2024-12-01 18:26:37 INFO: Loading: lemma\n",
      "2024-12-01 18:26:37 INFO: Loading: depparse\n",
      "2024-12-01 18:26:38 INFO: Loading: ner\n",
      "2024-12-01 18:26:39 INFO: Done loading processors!\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\language.py:1040: UserWarning: Due to multiword token expansion or an alignment issue, the original text has been replaced by space-separated expanded tokens.\n",
      "  doc = self._ensure_doc(text)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\language.py:1040: UserWarning: Can't set named entities because of multi-word token expansion or because the character offsets don't map to valid tokens produced by the Stanza tokenizer:\n",
      "Words: ['القطط', 'هي', 'حيوانات', 'أليفة', 'ل', 'طيفة', 'و', 'محبوبة', 'تعيش', 'مع', 'الإنسان', 'منذ', 'آلاف', 'السنين', '.', 'تتميز', 'القطط', 'ب', 'رشاقة', 'ها', 'و', 'ذكاء', 'ها', '،', 'و', 'لدي', 'ها', 'قدرة', 'رائعة', 'على', 'التكيف', 'مع', 'البيئة', 'المحيطة', '.', 'كما', 'تحب', 'اللعب', 'و', 'الاستكشاف', '،', 'و', 'تعتبر', 'الرفيق', 'المثالي', 'ل', 'من', 'يبحث', 'عن', 'صديق', 'هادئ', 'و', 'ممتع', 'في', 'نفس', 'الوقت', '.']\n",
      "Entities: []\n",
      "  doc = self._ensure_doc(text)\n",
      "2024-12-01 18:27:02 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f51849357a4700811b65e9cd547a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 18:27:06 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-12-01 18:27:06 WARNING: GPU requested, but is not available!\n",
      "2024-12-01 18:27:06 INFO: Using device: cpu\n",
      "2024-12-01 18:27:06 INFO: Loading: tokenize\n",
      "2024-12-01 18:27:06 INFO: Loading: mwt\n",
      "2024-12-01 18:27:06 INFO: Loading: pos\n",
      "2024-12-01 18:27:06 INFO: Loading: lemma\n",
      "2024-12-01 18:27:06 INFO: Loading: depparse\n",
      "2024-12-01 18:27:07 INFO: Loading: ner\n",
      "2024-12-01 18:27:08 INFO: Done loading processors!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# app Class\n",
    "class App(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.init_ui()\n",
    "\n",
    "    def init_ui(self):\n",
    "        # main window \n",
    "        self.setWindowTitle(\"lexico-semantic processing application\")\n",
    "        self.setGeometry(100, 100, 900, 600)\n",
    "        self.setStyleSheet(\"background-color: #FFE4E1;\")  \n",
    "\n",
    "        \n",
    "        central_widget = QWidget()\n",
    "        self.setCentralWidget(central_widget)\n",
    "\n",
    "        # main Layout\n",
    "        main_layout = QVBoxLayout()\n",
    "\n",
    "        # input text\n",
    "        input_layout = QHBoxLayout()\n",
    "        self.text_input = QLineEdit()\n",
    "        self.text_input.setPlaceholderText(\" input something ...\")\n",
    "        self.text_input.setStyleSheet(\"\"\"\n",
    "            QLineEdit {\n",
    "                background-color: #FFFFFF;\n",
    "                border: 2px solid #FFB6C1;\n",
    "                border-radius: 10px;\n",
    "                padding: 5px;\n",
    "                font-size: 16px;\n",
    "            }\n",
    "        \"\"\")\n",
    "        input_layout.addWidget(QLabel(\"input text:\", self))\n",
    "        input_layout.addWidget(self.text_input)\n",
    "        main_layout.addLayout(input_layout)\n",
    "\n",
    "        # upload file btn\n",
    "        self.upload_button = QPushButton(\"upload file\")\n",
    "        self.upload_button.setStyleSheet(\"\"\"\n",
    "            QPushButton {\n",
    "                background-color: #FFB6C1;\n",
    "                color: white;\n",
    "                border-radius: 15px;\n",
    "                padding: 10px 20px;\n",
    "                font-size: 14px;\n",
    "            }\n",
    "            QPushButton:hover {\n",
    "                background-color: #FFA07A;\n",
    "            }\n",
    "        \"\"\")\n",
    "        self.upload_button.clicked.connect(self.upload_file)\n",
    "        main_layout.addWidget(self.upload_button, alignment=Qt.AlignCenter)\n",
    "\n",
    "        #  area to ^print results \n",
    "        self.result_display = QTextEdit()\n",
    "        self.result_display.setReadOnly(True)\n",
    "        self.result_display.setPlaceholderText(\"results...\")\n",
    "        self.result_display.setStyleSheet(\"\"\"\n",
    "            QTextEdit {\n",
    "                background-color: #FFF5F5;\n",
    "                border: 2px solid #FFB6C1;\n",
    "                border-radius: 10px;\n",
    "                font-size: 14px;\n",
    "            }\n",
    "        \"\"\")\n",
    "        main_layout.addWidget(self.result_display)\n",
    "\n",
    "        # boxes with buttons >>>>>>>>>> each box wil cover a level of analysis\n",
    "        small_boxes_layout = QHBoxLayout()\n",
    "\n",
    "        # box 1: ** Morphological Analysis **\n",
    "        box1_layout = QVBoxLayout()\n",
    "        box1_label = QLabel(\"Morphological Analysis\")\n",
    "        box1_label.setFont(QFont(\"Arial\", 12, QFont.Bold))\n",
    "        box1_label.setStyleSheet(\"color: #FF69B4;\")\n",
    "        box1_label.setAlignment(Qt.AlignCenter)\n",
    "        box1_layout.addWidget(box1_label)\n",
    "\n",
    "        button1_1 = QPushButton(\"Segmentation\")\n",
    "        button1_1.clicked.connect(self.segmentatin)\n",
    "        button1_2 = QPushButton(\"Morphemes Dictionary\")\n",
    "        button1_2.clicked.connect(self.morphemes)\n",
    "        button1_3 = QPushButton(\"POS tag\")\n",
    "        button1_3.clicked.connect(self.pos_tags)\n",
    "        button1_4 = QPushButton(\"syntax dependencies\")\n",
    "        button1_4.clicked.connect(self.pos_tags)\n",
    "\n",
    "        # styling w adding buttons\n",
    "        for button in [button1_1, button1_2, button1_3, button1_4]:\n",
    "            button.setStyleSheet(\"\"\"\n",
    "                QPushButton {\n",
    "                    background-color: #FFB6C1;\n",
    "                    color: white;\n",
    "                    border-radius: 15px;\n",
    "                    padding: 5px 10px;\n",
    "                    font-size: 12px;\n",
    "                }\n",
    "                QPushButton:hover {\n",
    "                    background-color: #FFA07A;\n",
    "                }\n",
    "            \"\"\")\n",
    "            box1_layout.addWidget(button)\n",
    "\n",
    "        small_boxes_layout.addLayout(box1_layout)\n",
    "\n",
    "        \n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "        # add layouts to the main layout\n",
    "        main_layout.addLayout(small_boxes_layout)\n",
    "        central_widget.setLayout(main_layout)\n",
    "\n",
    "        #initialize a variables to hold file content\n",
    "        self.text = \"\"\n",
    "\n",
    "    def upload_file(self):\n",
    "        \"\"\"Open a file dialog to select and read a file.\"\"\"\n",
    "        file_path, _ = QFileDialog.getOpenFileName(self, \"Open File\", \"\", \"Text Files (*.txt);;All Files (*)\")\n",
    "        if file_path:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    self.text = file.read()\n",
    "                    self.result_display.setText(\"file uploaded successfully!\\n\\n\" + self.text)\n",
    "           \n",
    "\n",
    "#I . morphological analysis\n",
    "     # 1 segmentation \n",
    "    def segmentatin(self):\n",
    "        if self.text:\n",
    "                segmented = segmenter.segment(self.text)\n",
    "                self.result_display.setText(segmented)\n",
    "\n",
    "        else:\n",
    "            self.result_display.setText(\"no text to segment\")\n",
    "\n",
    "    #2  morphemes dictionary\n",
    "    def morphemes(self):\n",
    "        if self.text:\n",
    "            try:\n",
    "                nlp = spacy_stanza.load_pipeline(\"ar\")\n",
    "                \n",
    "                doc = nlp(self.text)\n",
    "\n",
    "                #  morphological information\n",
    "                morph_data = [f\"Token: {token.text}, Morph: {token.morph}\" for token in doc]\n",
    "                # Display the results in the QTextEdit\n",
    "                self.result_display.setText(\"\\n\".join(morph_data))\n",
    "            except Exception as e:\n",
    "                self.result_display.setText(\"error in collecting morphological information \")\n",
    "        else:\n",
    "             self.result_display.setText(\"no text to analyze!\")\n",
    "\n",
    "\n",
    "    def pos_tags(self):\n",
    "        if self.text:  \n",
    "            try:\n",
    "                pos_tagger = FarasaPOSTagger(interactive=True)  # initialize farasa POS tagger\n",
    "                #tokenization\n",
    "                tokens = word_tokenize(self.text)\n",
    "                # tag each token\n",
    "                tags = []\n",
    "                for token in tokens:\n",
    "                    tag = pos_tagger.tag(token)  # This returns a tuple like ('word', 'POS_tag')\n",
    "                    tags.tag(token)\n",
    "                    # display\n",
    "                    self.result_display.setText(\"\\n\".join(tags))\n",
    "            except Exception as e:\n",
    "                  self.result_display.setText(\"error in tagging\")\n",
    "        else:\n",
    "            self.result_display.setText(\"no text to tag\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def clear_result(self):\n",
    "        self.result_display.clear()\n",
    " \n",
    "\n",
    "    def count_words(self):\n",
    "        if self.text:\n",
    "            word_count = len(self.uploaded_content.split())\n",
    "            self.result_display.setText(\" nu,ber of word:\", word_count)\n",
    "        else:\n",
    "            self.result_display.setText(\"no content to count 🙁\")\n",
    "\n",
    "\n",
    "# tun the app\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    window = App()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e516871-4e4b-471a-ac8c-68bf4a546afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
